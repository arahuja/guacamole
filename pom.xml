<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">

  <parent>
    <groupId>org.hammerlab</groupId>
    <artifactId>scala-parent-pom</artifactId>
    <version>1.0.3</version>
    <relativePath />
  </parent>

  <modelVersion>4.0.0</modelVersion>
  <artifactId>guacamole</artifactId>
  <version>0.0.1-SNAPSHOT</version>
  <packaging>jar</packaging>
  <name>guacamole: variant caller</name>

  <properties>
    <github.repo>hammerlab/guacamole</github.repo>
    <scala.version>2.10.6</scala.version>
    <scala.version.prefix>2.10</scala.version.prefix>
    <hadoop.version>2.7.0</hadoop.version>
    <spark.version>1.6.1</spark.version>
    <spark.hadoop.scope>provided</spark.hadoop.scope>
  </properties>

  <build>
    <!--
      The Guacamole JAR output by maven-jar-plugin, at the path configured here, can't run on its own; it needs some
      dependency-classes shaded in, as configured elsewhere in this file. However, the shade plugin requires it to be
      present in order to work.

      As such, we write it to a non-default path here, and treat the minimum-viable Guacamole+shaded-deps JAR (built by
      the default "guac" profile below) as the primary deliverable.
     -->
    <finalName>guacamole-unshaded-${project.version}</finalName>
    <plugins>
      <plugin>
        <groupId>org.scalatest</groupId>
        <artifactId>scalatest-maven-plugin</artifactId>
        <configuration>
          <!-- Bump the memory used by scalatest. -->
          <argLine>-Xmx4g</argLine>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <version>2.4.3</version>
        <configuration>
          <transformers>
            <!-- Set "main" class. -->
            <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
              <mainClass>org.hammerlab.guacamole.Main</mainClass>
            </transformer>
          </transformers>

          <filters>
            <!-- Exclude signature files that we don't validate. -->
            <filter>
              <artifact>*:*</artifact>
              <excludes>
                <exclude>META-INF/*.SF</exclude>
                <exclude>META-INF/*.DSA</exclude>
                <exclude>META-INF/*.RSA</exclude>
                <exclude>META-INF/LICENSE</exclude>
                <exclude>META-INF/license</exclude>
              </excludes>
            </filter>
          </filters>

          <relocations>
            <!--
              We use some Guava classes and APIs that didn't show up until Guava 14.0.1, but Hadoop gives us Guava
              11.0.2 at runtime, so we shade and rename the classes we care about (com.google.common.collect.**) here,
              along with two other packages that its APIs reference (…base.** and …primitives.**).
            -->
            <relocation>
              <pattern>com.google.common</pattern>
              <shadedPattern>org.hammerlab.guava</shadedPattern>
              <includes>
                <include>com.google.common.collect.**</include>
                <include>com.google.common.base.**</include>
                <include>com.google.common.primitives.**</include>
              </includes>
            </relocation>

            <!--
              Spark (MLLib) depends on Breeze 0.11.2 and we depend on 0.12. By Spark 2.0.1 we should be able to
              remove this.
            -->
            <relocation>
              <pattern>breeze</pattern>
              <shadedPattern>org.hammerlab.breeze</shadedPattern>
              <includes>
                <include>breeze.**</include>
              </includes>
            </relocation>
          </relocations>
        </configuration>
      </plugin>
    </plugins>
  </build>

  <dependencies>
    <dependency>
      <groupId>com.esotericsoftware.kryo</groupId>
      <artifactId>kryo</artifactId>
      <version>2.21</version>
    </dependency>
    <dependency>
      <groupId>args4j</groupId>
      <artifactId>args4j</artifactId>
      <version>[2.0.29,)</version>
    </dependency>
    <dependency>
      <groupId>org.hammerlab.adam</groupId>
      <artifactId>adam-core_${scala.version.prefix}</artifactId>
      <version>0.20.0</version>
    </dependency>
    <dependency>
      <groupId>org.bdgenomics.bdg-formats</groupId>
      <artifactId>bdg-formats</artifactId>
      <version>0.9.0</version>
    </dependency>
    <dependency>
      <groupId>org.bdgenomics.quinine</groupId>
      <artifactId>quinine-core_${scala.version.prefix}</artifactId>
      <version>0.0.2</version>
      <exclusions>
        <exclusion>
          <groupId>org.bdgenomics.adam</groupId>
          <artifactId>adam-core_2.10</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.spire-math</groupId>
      <artifactId>spire_${scala.version.prefix}</artifactId>
      <version>0.11.0</version>
    </dependency>
    <dependency>
      <groupId>org.scalanlp</groupId>
      <artifactId>breeze_${scala.version.prefix}</artifactId>
      <version>0.12</version>
    </dependency>
    <dependency>
      <groupId>com.github.samtools</groupId>
      <artifactId>htsjdk</artifactId>
      <version>2.6.1</version>
      <exclusions>
        <!--
          htsjdk:2.6.1 brings in snappy-java:1.0.3-rc3, but running tests on OSX requires ≥ 1.0.5, per
          http://stackoverflow.com/a/31482848/544236.
        -->
        <exclusion>
          <groupId>org.xerial.snappy</groupId>
          <artifactId>snappy-java</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.seqdoop</groupId>
      <artifactId>hadoop-bam</artifactId>
      <version>7.6.0</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-common</artifactId>
      <version>1.8.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-encoding</artifactId>
      <version>1.8.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-column</artifactId>
      <version>1.8.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.parquet</groupId>
      <artifactId>parquet-hadoop</artifactId>
      <version>1.8.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.commons</groupId>
      <artifactId>commons-math3</artifactId>
      <version>3.0</version>
    </dependency>
    <dependency>
      <groupId>com.google.guava</groupId>
      <artifactId>guava</artifactId>
      <version>18.0</version>
    </dependency>
    <dependency>
      <groupId>org.hammerlab</groupId>
      <artifactId>magic-rdds_${scala.version.prefix}</artifactId>
      <version>1.2.4</version>
    </dependency>
    <dependency>
      <groupId>org.clapper</groupId>
      <artifactId>grizzled-slf4j_${scala.version.prefix}</artifactId>
      <version>1.0.3</version>
    </dependency>

    <dependency>
      <groupId>org.apache.hadoop</groupId>
      <artifactId>hadoop-client</artifactId>
      <version>${hadoop.version}</version>
      <scope>${spark.hadoop.scope}</scope>
      <exclusions>
        <!--
          hadoop-client's javax.servlet:servlet-api dependency includes classes that conflict with spark-core's
          org.eclipse.jetty.orbit:javax.servlet dependency; excluding the former resolves this, cf.
          https://issues.apache.org/jira/browse/SPARK-1693.
        -->
        <exclusion>
          <groupId>javax.servlet</groupId>
          <artifactId>*</artifactId>
        </exclusion>
      </exclusions>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.version.prefix}</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.hadoop.scope}</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-mllib_${scala.version.prefix}</artifactId>
      <version>${spark.version}</version>
      <scope>${spark.hadoop.scope}</scope>
    </dependency>

    <!-- Test deps -->
    <dependency>
      <groupId>com.holdenkarau</groupId>
      <artifactId>spark-testing-base_${scala.version.prefix}</artifactId>
      <version>${spark.version}_0.4.4</version>
      <scope>test</scope>
    </dependency>
    <dependency>
      <groupId>org.bdgenomics.bdg-utils</groupId>
      <artifactId>bdg-utils-misc</artifactId>
      <version>0.1.1</version>
      <type>test-jar</type>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <profiles>
    <profile>
      <!--
        Default profile: build a Guacamole jar with shaded Guava and Breeze, and write transitive-dependency-JAR paths
        to target/dependencies (comma-delimited to serve the common case of passing the values to Spark's "jars" flag).
      -->
      <id>guac</id>
      <activation>
        <activeByDefault>true</activeByDefault>
      </activation>
      <build>
        <!--
          `mvn install` will look for a JAR with this name, which is taken to be the main deliverable JAR of this
          project.

          We name it here in a manner that matches what we tell the shade-plugin to output below.
        -->
        <finalName>guacamole-${project.version}</finalName>
        <plugins>
          <plugin>
            <artifactId>maven-shade-plugin</artifactId>
            <executions>
              <execution>
                <id>guac</id>
                <goals>
                  <goal>shade</goal>
                </goals>
                <configuration>
                  <finalName>guacamole-${project.version}</finalName>
                  <!--
                    Don't "attach" this shaded JAR to the project (in addition to the default unshaded JAR that would be
                    produced by the jar-plugin; this JAR *is* the deliverable of the project.
                  -->
                  <shadedArtifactAttached>false</shadedArtifactAttached>
                  <artifactSet>
                    <includes>
                      <include>org.hammerlab.guacamole:*</include>
                      <include>com.google.guava:*</include>
                      <include>org.scalanlp:breeze_${scala.version.prefix}</include>
                    </includes>
                  </artifactSet>
                </configuration>
              </execution>
            </executions>
          </plugin>

          <!-- Write a file with paths to all transitive-dependencies to target/dependencies. -->
          <plugin>
            <groupId>org.apache.maven.plugins</groupId>
            <artifactId>maven-dependency-plugin</artifactId>
            <version>2.10</version>
            <executions>
              <execution>
                <goals>
                  <goal>build-classpath</goal>
                </goals>
                <configuration>
                  <outputFile>${target.dir}/dependencies</outputFile>

                  <!-- Use commas for ease of passing to spark-{submit,shell}'s "jars" flag. -->
                  <pathSeparator>,</pathSeparator>

                  <!-- Leave Guava and Breeze out, as they are included in the Guacamole JAR defined above. -->
                  <excludeArtifactIds>
                    guava,breeze_${scala.version.prefix}
                  </excludeArtifactIds>
                </configuration>
              </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
    </profile>

    <!--
      "uber" profile: build a JAR with Guacamole and all transitive dependencies shaded, and Guava and Breeze relocated.
    -->
    <profile>
      <id>uber</id>
      <build>
        <plugins>
          <plugin>
            <artifactId>maven-shade-plugin</artifactId>
            <executions>
              <execution>
                <id>assembly</id>
                <goals>
                  <goal>shade</goal>
                </goals>
                <configuration>
                  <finalName>guacamole-assembly-${project.version}</finalName>
                </configuration>
              </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
    </profile>

    <profile>
      <!--
        "deps" profile: build a JAR with just Guacamole's transitive dependencies, excluding those that are shaded into
        the Guacamole JAR itself.
      -->
      <id>deps</id>
      <build>
        <plugins>
          <plugin>
            <artifactId>maven-shade-plugin</artifactId>
            <executions>
              <execution>
                <id>deps</id>
                <goals>
                  <goal>shade</goal>
                </goals>
                <configuration>
                  <finalName>guacamole-deps-${project.version}</finalName>
                  <transformers combine.children="append">
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer" />
                  </transformers>
                  <filters combine.children="append">
                    <!-- Leave all Guacamole classes out of this JAR. -->
                    <filter>
                      <artifact>*:guacamole</artifact>
                      <excludes>
                        <exclude>org/hammerlab/guacamole/**</exclude>
                      </excludes>
                    </filter>
                  </filters>
                  <artifactSet>
                    <excludes>
                      <!-- These are shaded into the "guac"-profile JAR. -->
                      <exclude>com.google.guava:*</exclude>
                      <exclude>org.scalanlp:breeze_${scala.version.prefix}</exclude>
                    </excludes>
                  </artifactSet>
                </configuration>
              </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
    </profile>

    <profile>
      <!--
        "local" profile puts Spark and Hadoop into "compile" scope (rather than "provided"); generally not desirable,
        unless intending to run Guacamole directly from the command-line using the `java` executable, instead of
        spark-{submit,shell}.
      -->
      <id>local</id>
      <properties>
        <spark.hadoop.scope>compile</spark.hadoop.scope>
      </properties>
      <build>
        <plugins>
          <plugin>
            <artifactId>maven-shade-plugin</artifactId>
            <configuration>
              <transformers combine.children="append">
                <!-- Akka (which Spark depends on) needs this reference.conf munging in order to work correctly in -->
                <!-- shaded JARs, cf. http://stackoverflow.com/questions/31011243/#31011315 -->
                <transformer implementation="org.apache.maven.plugins.shade.resource.AppendingTransformer">
                  <resource>reference.conf</resource>
                </transformer>

                <!-- Necessary for Hadoop to load local files correctly, cf. http://stackoverflow.com/a/27532248/544236 -->
                <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer" />
              </transformers>
            </configuration>
          </plugin>
        </plugins>
      </build>
    </profile>

    <profile>
      <id>test</id>
      <build>
        <plugins>
          <plugin>
            <artifactId>maven-jar-plugin</artifactId>
            <executions>
              <execution>
                <id>test-jar</id>
                <phase>package</phase>
                <goals>
                  <goal>test-jar</goal>
                </goals>
              </execution>
            </executions>
          </plugin>
        </plugins>
      </build>
    </profile>

  </profiles>
</project>
